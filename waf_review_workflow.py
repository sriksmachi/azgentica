import os
import json
from typing_extensions import TypedDict, Annotated
from langchain_core.messages import AnyMessage
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.graph.message import add_messages
from dotenv import load_dotenv
from langchain_community.chat_models import AzureChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.types import Command
from pyexpat.errors import messages
from dotenv import load_dotenv
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import InMemorySaver
from langgraph_supervisor import create_supervisor
from langgraph.prebuilt import create_react_agent
from langchain_openai import AzureChatOpenAI
from prompts import data_extraction_prompt, cost_calculation_prompt
from langchain_core.messages import HumanMessage
from image_data_extraction import encode_image
from langgraph.graph import MessagesState, StateGraph, START, END
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage
from image_data_extraction import create_prompt_with_image
from typing import Literal
from typing_extensions import TypedDict
from langgraph.graph import MessagesState, START, END
from langgraph.types import Command


class ServiceRecommendations(TypedDict):
    service_name: str
    review: str
    recommedation: str


class GraphState(TypedDict):
    messages: list[AnyMessage]
    uploaded_image: str | None
    services: list[str] | None
    service_recommendations: list[ServiceRecommendations]
    summary: str | None
    total_iterations: int
    pillar_in_review: str | None


load_dotenv()

llm_client = AzureChatOpenAI(
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
    api_version="2024-08-01-preview",
    temperature=0.3,
    model_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
)


def summarize_results(state: MessagesState) -> MessagesState:
    """Summarize the results of the architecture review."""
    messages = state['messages'][0]
    new_message = [
        HumanMessage(
            content=[
                {"type": "text",
                 "text": "Summarize the results of the architecture review."},
            ]
        )
    ]
    result = llm_client.invoke(messages + new_message)
    return Command(
        update={
            "messages": [
                HumanMessage(content=result.content)
            ]
        }
    )


def supervisor_node(state: GraphState):
    system_prompt = f"""You are a supervisor agent facilitating an Architecture review board of an Azure Architecture.
Your task is to make sure the review of the architecture is done in a structured manner by choosing the next agent, and the pillar to review. \
The review should be done around the Azure Well-Architected Framework(WAF) pillars. \
The output for each pillar should have the recommendations and the checklist for the architecture review. \
The review should be done around the following pillars of Azure Well-Architected Framework:

1. ** Cost Optimization**: Ensure the architecture is cost-effective and efficient.
2. ** Operational Excellence**: Ensure the architecture is reliable and maintainable.
3. ** Performance Efficiency**: Ensure the architecture is performant and scalable.
4. ** Reliability**: Ensure the architecture is resilient and can recover from failures.
5. ** Security**: Ensure the architecture is secure and compliant with best practices.

**State Definition**:
- `messages`: List of messages exchanged in the conversation.
- `uploaded_image`: The image of the architecture diagram uploaded by the user.
- `cost_optimization_review`: ReviewState object containing the review for the Cost Optimization pillar.
- `operational_excellence_review`: ReviewState object containing the review for the Operational Excellence pillar.
- `performance_efficiency_review`: ReviewState object containing the review for the Performance Efficiency pillar.
- `reliability_review`: ReviewState object containing the review for the Reliability pillar.
- `security_review`: ReviewState object containing the review for the Security pillar.
- `summary`: Summary of the architecture review.
- `total_iterations`: Total number of iterations in the workflow.

**Agents**:
1. ** Data Generator**: This agent will generate a response to the questionaire and checklist based on the uploaded image and its summary.
2. ** ArchitectureReviewer**: This agent will generate the questionnaire and checklist for the architecture review based on the uploaded image.
3. ** Evaluator**: This agent will evaluate the content generated by Data Generator against the questionnaire and checklist and evalute if the review is complete or continue with more iterations.
4. ** Summarizer**: This agent will summarize the results of the architecture review and provide a final summary of the architecture review.

Check the StateGraph to make sure the pillars are being followed. Route the messages to the Evaluator to determine the next step in the workflow.
Evaluator willl review the content generated by the agents and decide the next step in the workflow.

**Current State**:
Cost Optimization Review: {state['cost_optimization_review']}
Operational Excellence Review: {state['operational_excellence_review']}
Performance Efficiency Review: {state['performance_efficiency_review']}
Reliability Review: {state['reliability_review']}
Security Review: {state['security_review']}
Summary: {state['summary']}
Total Iterations: {state['total_iterations']}

Response Format:
```"next": "supervisor" | "data_generator" | "reviewer" | "evaluator" | "summarizer" | "FINISH",
   "pillar": "cost_optimization" | "operational_excellence" | "performance_efficiency" | "reliability" | "security"
```
"""
    new_message = [
        {"role": "system", "content": system_prompt},
    ]
    state['messages'] = new_message + state['messages']
    response = llm_client.with_structured_output(
        Router).invoke(state["messages"])
    goto = response["next"]
    pillar = response.get("pillar", None)
    print(f"Next Worker: {goto}, assigned pillar: {pillar}")
    state["pillar_in_review"] = pillar
    if goto == "FINISH":
        goto = END
        state["pillar_in_review"] = None
    return Command(goto=goto, update={
        "state": state,
        "messages":
            [HumanMessage(content=f"Next pillar to review: {pillar}"
                          )]})


def evaluator_agent(state: GraphState):
    """Evaluate the content generated by the agents and decide the next step in the workflow."""
    messages = state['messages'][0]
    new_message = [
        HumanMessage(
            content=[
                {"type": "text",
                 "text": "Evaluate the content generated by the agents and decide the next step in the workflow."},
            ]
        )
    ]
    result = llm_client.invoke(messages + new_message)
    return Command(
        update={
            "messages": [
                HumanMessage(content=result.content)
            ]
        }
    )


# Define the workflow for the architecture review process
review_workflow = StateGraph(GraphState)
# review_workflow.add_node("generator", document_generator_agent)
# review_workflow.add_node("ArchitectureReviewer", ArchitectureReviewer_agent)
review_workflow.add_node("supervisor", supervisor_node)
review_workflow.add_node("reviewer", architecture_reviewer_agent)
# review_zworkflow.add_conditional_edge(
#     "ArchitectureReviewer",
#     evaluator_agent,
#     {
#         "generator": document_generator_agent,
#         "ArchitectureReviewer": ArchitectureReviewer_agent,
#         "summarize_results": summarize_results,
#     })
review_workflow.add_edge(START, "supervisor")
graph = review_workflow.compile()


if __name__ == "__main__":
    # Example usage
    state = {
        "cost_optimization_review": None,
        "operational_excellence_review": None,
        "performance_efficiency_review": None,
        "reliability_review": None,
        "security_review": None,
        "summary": None,
        "total_iterations": 0,
        "messages": [
            HumanMessage(
                content=[
                    {"type": "text", "text": "Review the architecture diagram."}
                ]
            )
        ]
    }
    for message in graph.stream(state):
        print(message)
